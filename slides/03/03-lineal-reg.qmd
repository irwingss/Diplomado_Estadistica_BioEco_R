---
title: Modelamiento Lineal
subtitle: Módulo 7
title-slide-attributes:
  data-background-image: ../bg4.png
  data-background-size: cover
  data-slide-number: default
format:
  revealjs:
    theme:
     - "default"
     - "../slides.scss"
    width: "1600"
    height: "900"
    slide-number: true
    highlight-style: atom-one
    incremental: true   
    chalkboard: 
       theme: chalkboard
       chalk-effect: 0.1
       chalk-width: 5
       boardmarker-width: 7
filters:
  - shinylive
cache: true
---

```{r, echo=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  fig.align = "center"
)
```

# Objetivos del módulo {background-image="../bg8.png"}

::: incremental
1. Comprender los **fundamentos del modelamiento lineal**, aplicaciones y limitaciones.
2. Analizar el cumplimiento de los **supuestos teóricos** en modelos lineales.
3. **Interpretar** de manera correcta los resultados numéricos de un modelo lineal.
4. Aplicar procesos de selección de variables en modelos lineales múltiples.
5. Desarrollar scripts de modelamiento lineal de manera efectiva con sintaxis de R.
:::

# Introducción

## Temario del módulo

::: incremental
- Concepto de modelo lineal y función matemática.
- Aplicaciones y limitaciones.
- Supuestos teóricos del modelo lineal.
- Implementación de modelos lineales simples.
- Coeficiente de determinación.
- Estimación de parámetros: OLS vs Maximum Likelihood.
- Interpretación para variables explicativas numéricas y factores.
- Regresión lineal múltiple: interpretación.
- Concepto de interacción.
- Selección de variables: AIC.
- Regresiones polinómicas y transformaciones.
- Modelos Log.
- Posibilidades analíticas si no se cumplen los supuestos teóricos.
:::

# 1. Modelos lineales

"La generalización de la realidad más utilizada. Pero ¿es la más adecuada?"

## Modelo lineal simple

Es la forma más simple de modelar la realidad.

$$
y = \beta_0 + \beta_1 x_1 + \epsilon
$$

Donde:

<br>

- $y$ es la variable dependiente.
- $x_1$ es la variable independiente.
- $\beta_0$ es el coeficiente que representa el intercepto de la recta en $y$.
- $\beta_1$ es el coeficiente que representa la pendiente de la recta.
- $\epsilon$ es el término de error, que captura la variabilidad no explicada por el modelo.

## {}

 
```{shinylive-r}
#| standalone: true
#| viewerHeight: 700

# Cargar las librerías necesarias
library(shiny)
library(ggplot2)

# Definir la interfaz de usuario
ui <- fluidPage(
  titlePanel("Visualización Interactiva de un Modelo Lineal"),
  
  sidebarLayout(
    sidebarPanel(
      sliderInput("beta1", "β₁ (Pendiente):", min = -5, max = 5, value = 1, step = 0.1),
      sliderInput("beta0", "β₀ (Intercepto):", min = -10, max = 10, value = 0, step = 0.5),
      actionButton("reset", "Restablecer Valores")
    ),
    
    mainPanel(
      plotOutput("linePlot"),
      br(),
      h4("Ecuación de la Recta:"),
      uiOutput("equation")
    )
  )
)

# Definir la lógica del servidor
server <- function(input, output, session) {
  
  # Generar datos sintéticos
  set.seed(123)  # Para reproducibilidad
  x_data <- seq(1, 10, length.out = 100)
  true_beta1 <- 2
  true_beta0 <- 1
  noise <- rnorm(100, mean = 0, sd = 2)
  y_data <- true_beta1 * x_data + true_beta0 + noise
  data <- data.frame(X = x_data, Y = y_data)
  
  # Reactivo para restablecer los valores
  observeEvent(input$reset, {
    updateSliderInput(session, "beta1", value = 1)
    updateSliderInput(session, "beta0", value = 0)
  })
  
  # Crear el gráfico
  output$linePlot <- renderPlot({
    ggplot(data, aes(x = X, y = Y)) +
      geom_point(color = "blue") +
      geom_abline(intercept = input$beta0, slope = input$beta1, color = "red", size = 1) +
      labs(title = "Datos Sintéticos y Recta Ajustable",
           x = "Variable Independiente (X)",
           y = "Variable Dependiente (Y)") +
      theme_minimal()
  })
  
  # Mostrar la ecuación de la recta con notación matemática
  output$equation <- renderUI({
    eq <- paste0("Y = ", input$beta0, " + ", input$beta1, " × X")
    withMathJax(paste0("$$", eq, "$$"))
  })
}

# Ejecutar la aplicación
shinyApp(ui = ui, server = server)


```
 
## Modelo lineal múltiple

Implica tener más de una variable independiente. Cada una con su respectivo coeficiente $\beta_i$:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n + \epsilon
$$

Donde:

<br>

- $y$ es la variable dependiente.
- $x_1, x_2, \dots, x_n$ son las variables independientes.
- $\beta_0$ es el intercepto de la curva.
- $\beta_1, \beta_2, \dots, \beta_n$ son los coeficientes que representan la pendiente de cada variable.
- $\epsilon$ es el término de error, que captura la variabilidad no explicada por el modelo.

##  {.center .dark-background background-image="../landscape1.jpg"}

::: task
En ecología, los modelos lineales ayudan a entender las relaciones entre mediciones de factores bioticos/abióticos (como temperatura, luz, radiación, o humedad, densidad poblacional, cobertura vegetal, densidad del depredador) y las respuestas biológicas de las especies.
:::

## {}

:::{.fragment}
Predecir la **biomasa** de una comunidad vegetal en función de la **cobertura vegetal** del suelo.

$$
\text{Biomasa} = \beta_0 + \beta_1\cdot\text{Cobertura Vegetal}
$$
:::

:::{.fragment}
Se puede aproximar la riqueza de especies frente a la extensión del hábitat (basado en la teoría de la biogeografía de islas) con un modelo lineal log-transformado:

$$
\log_e{(\text{Riqueza})}= \beta_0 + \beta_1\cdot\log_e{(\text{Área})}
$$
:::

:::{.fragment}
Analizar cómo la concentración de **nutrientes** (nitrógeno o fósforo) influye en la métrica de **diversidad de especies** de macroinvertebrados en un río:

$$
\log_e{(\text{Riqueza})}= \beta_0 + \beta_1\cdot\log_e{(\text{Área})}
$$
:::

##  {.center .dark-background background-image="../landscape3.jpg"}

::: task
<strong>Recuerda:</strong><br>
Sin datos nada es posible. Estos modelos permiten eso: crear un modelo a partir de los datos para entender la relación de las variables o para futuras predicciones.
:::

## Aplicaciones y limitaciones

Siempre que la respuesta biológica, o la parte de nuestro interés, sea lineal, puedes utilizarlo.

:::::{.columns}
::::{.column}
```{r}
#| echo: false

knitr::include_graphics("images/linear relationship.png")
```

<br>

::: task
¿Es linear la relación entre la talla de los árboles y la biomasa?
:::

::::

::::{.column}
```{r}
#| echo: false

knitr::include_graphics("images/tallesttrees.png")
```
::::
:::::

## Allometric equations for integrating remote sensing imagery into forest monitoring programmes {}

::::{.columns}
:::{.column}
```{r}
#| echo: false

knitr::include_graphics("images/lidar.png")
```
:::

:::{.column}
```{r}
#| echo: false

knitr::include_graphics("images/lidar2.png")
```
:::
::::

::: footer
<https://onlinelibrary.wiley.com/doi/10.1111/gcb.13388>
:::

## Above-ground biomass models for dominant trees species in cacao agroforestry systems in Talamanca, Costa Rica {}

::::{.columns}
:::{.column}
```{r}
#| echo: false
#| out-width: 80%

knitr::include_graphics("images/cacao biomasa1.png")
```
:::

:::{.column}
```{r}
#| echo: false

knitr::include_graphics("images/cacao biomasa.png")
```
:::
::::

::: footer
<https://link.springer.com/article/10.1007/s10457-022-00741-y>
:::


## Supuestos teóricos del modelo lineal

- **Relación lineal**: Las variables independientes y la variable de respuesta tienen una relación lineal. 
- **Normalidad**: Los residuos se distribuyen normalmente. No es necesario que los predictores y el resultado estén distribuidos normalmente, siempre que los residuos lo estén. - **Homocedasticidad**: El término de error es el mismo para todos los valores de las variables independientes. 
- **No autocorrelación**: Los residuos o términos de error son independientes entre sí. 
- **No multicolinealidad**: Las variables independientes de un modelo de regresión no deben estar muy correlacionadas entre sí.
- **No valores influyentes**: Los datos no contienen valores atípicos que sean influyentes.

## Posibilidades analíticas al fallar asunciones teóricas

```{r}
#| echo: false


library(gt)
library(tibble)

# Crear el dataframe con las asunciones, modelos alternativos y transformaciones
datos_asunciones_regresion <- tibble(
  Asunción = c(
    "Relación Lineal",
    "Normalidad de Residuos",
    "Homocedasticidad",
    "No Autocorrelación",
    "No Multicolinealidad",
    "No Valores Influyentes"
  ),
  `Modelos Alternativos` = c(
    "• Regresión Polinomial • Regresión con Splines • Modelos No Lineales (e.g., exponenciales, logarítmicos) • Modelos de Machine Learning (Árboles de decisión, Random Forest, Redes Neuronales)",
    "• Modelos Lineales Generalizados (GLM) • Regresión Cuantílica (QR) • Modelos No Paramétricos (GAM)",
    "• Mínimos Cuadrados Ponderados (WLS)",
    "• Modelos de Efectos Mixtos, Data Panel Regression (DPR)",
    "• Regresión Ridge • Regresión Lasso • Regresión Elastic Net • Análisis de Componentes Principales (PCA) para obtener componentes principales",
    "• Regresión Robusta • Modelos No Paramétricos (GAM)"
  ),
  `Transformación de X` = c(
    "• Logaritmo • Raíz cuadrada • Exponencial",
    "",
    "• Opcional (e.g., transformación de X)",
    "",
    "• PCA • Combinación de variables",
    ""
  ),
  `Transformación de Y` = c(
    "• Logaritmo • Raíz cuadrada • Potencias (e.g., Box-Cox)",
    "• Logaritmo • Raíz cuadrada • Box-Cox",
    "• Logaritmo • Raíz cuadrada",
    "• Diferenciación • Logaritmo",
    "",
    ""
  )
)

# Crear la tabla con gt
tabla_asunciones_regresion <- datos_asunciones_regresion %>%
  gt() %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(
      columns = c(Asunción)
    )
  ) %>%
  cols_align(
    align = "left",
    columns = c(`Modelos Alternativos`, `Transformación de X`, `Transformación de Y`)
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) %>%
  opt_row_striping() %>%
  tab_options(
    column_labels.background.color = "#0013fe",
    table.font.size = px(25)
  ) %>%
  opt_css(
    css = "
    .gt_table tr:hover td {
      background-color: #E6F3FF !important;
    }
    "
  ) 

# Mostrar la tabla
tabla_asunciones_regresion

```



## Aplicación en R: Corregir Fallas en Relación Lineal

```{r}
#| eval: false

# Regresión lineal simple
modelo_lineal <- lm(Y ~ X, data = datos)

# Regresión polinomial de grado 2
modelo_polinomial <- lm(Y ~ poly(X, 2), data = datos)
```

```{r}
#| eval: false

# Regresión con Spline
library(splines)
modelo_splines <- lm(Y ~ bs(X, df = 3), data = datos)

# Modelo Exponencial: Y = β0 * exp(β1 * X) + ϵ
modelo_exponencial <- nls(Y ~ b0 * exp(b1 * X), data = datos,
                           start = list(b0 = 1, b1 = 0.05))
```

```{r}
#| eval: false

# Modelo Logarítmico: log(Y) = β0 + β1 * X + ϵ
modelo_logaritmico <- lm(log(Y) ~ X, data = datos)

# Ajustar una red neuronal con una capa oculta de 5 neuronas
library(nnet)
modelo_nn <- nnet(Y ~ X, data = datos, size = 5, linout = TRUE, trace = FALSE)

```

## Aplicación en R: Corregir Fallas en Normalidad de Residuos

```{r}
#| eval: false

# Modelo GLM con distribución normal (similar a lm)
modelo_glm_normal <- glm(Y ~ X, data = datos, family = gaussian())

# Modelo GLM con distribución Gamma (si los datos son positivos y asimétricos)
modelo_glm_gamma <- glm(Y ~ X, data = datos, family = Gamma(link = "log"))

# Regresión cuantílica para el cuantil 0.5 (mediana)
library(quantreg)
modelo_qr <- rq(Y ~ X, data = datos, tau = 0.5)

# Modelo GAM con término suave para X
library(mgcv)
modelo_gam <- gam(Y ~ s(X), data = datos)
```

## Aplicación en R: Corregir Fallas en Homocedasticidad (WLS)

```{r}
#| eval: false

# Ajustar un modelo OLS inicial
modelo_ols <- lm(Y ~ X, data = datos)

# Obtener residuos y analizar la varianza
datos <- datos %>%
  mutate(resid = resid(modelo_ols))

# Ajustar un modelo para la varianza utilizando GAM
modelo_gam_var <- gam(resid^2 ~ s(X), data = datos, family = gaussian())

# Predicciones de la varianza
datos <- datos %>%
  mutate(var_pred = predict(modelo_gam_var, 
                            newdata = datos),
         peso = 1 / var_pred)
```

```{r}
#| eval: false

# Modelo WLS
modelo_wls <- lm(Y ~ X, data = datos, weights = peso)
```

## Aplicación en R: Corregir Autocorrelación

```{r}
#| eval: false

# Modelo de efectos mixtos con intercepto aleatorio
modelo_mixto <- lmer(Y ~ X + (1 | grupo), data = datos)
```

## Aplicación en R: Corregir Multicolinearidad

```{r}
#| eval: false

library(glmnet)

# Modelo Ridge con validación cruzada. alpha = 0
cv_ridge <- cv.glmnet(x, y, alpha = 0)

# Modelo Lasso con validación cruzada. alpha = 1
cv_ridge <- cv.glmnet(x, y, alpha = 1)

# Modelo Elastic Net con validación cruzada. alpha = 0.5
cv_ridge <- cv.glmnet(x, y, alpha = 0.5)
```

## Aplicación en R: Corregir Valores Influyentes

```{r}
#| eval: false

# Modelo de regresión robusta
library(MASS)
modelo_robusto <- rlm(Y ~ X, data = datos)

# Modelo GAM con término suave para X
library(mgcv)
modelo_gam_robusto <- gam(Y ~ s(X), data = datos)

```

## ¿Cómo se estiman los parámetros?: Optimización

::: nonincremental

- **Mínimos Cuadrados Ordinarios (OLS)**: Es el método más utilizado en Reg. Lineal. OLS busca minimizar la suma de los cuadrados de las diferencias (residuos) entre los valores observados y los valores predichos por el modelo.

- **Máxima Verosimilitud (MLE)**: Este método se basa en maximizar la función de verosimilitud, que representa la probabilidad de observar los datos dados los parámetros del modelo.

:::

```{r}
#| echo: false

knitr::include_graphics("images/mle.png")
```


## Resultados de los modelos 1:

```{r}
#| echo: false
summary(lm(Petal.Length ~ Sepal.Length + Species, data = iris))
```

- Residuals: 
- Coefficients: Pendiente $\beta$ de cada variable y el intercepto $\alpha$ de la recta.
- $p_{value}$ de los coeficientes: para evaluar la $H_0$ "no existe efecto de la variable $X$". $p_{value} < 0.05$, variable significativa, se rechaza $H_0$, hay efecto.

## Resultados de los modelos 2:

```{r}
#| echo: false
summary(lm(Petal.Length ~ Sepal.Length + Species, data = iris))
```

- $RSE$ o Error estándar residual: 
- $R^2$ o coeficiente de determinación: proporción de la varianza de $Y$ explicada por las variables $X$. Usar el penalizado $Adj. R^2$.
- $p_{value}$ del modelo: para evaluar la $H_0$ "el modelo no explica la variable $Y$". $p_{value} < 0.05$, modelo significativo.

## Modelo con interacciones

En modelos de regresión, un término de interacción **captura el efecto combinado de dos o más variables independientes sobre la variable dependiente**. Es decir, el efecto de una variable independiente $X$ sobre $Y$ depende del valor de otra variable independiente.

#### ¿Cuando Incorporar Interacciones?

- **Dependencia de Efectos**: Si se sospecha que el efecto de una variable sobre la respuesta varía según otra variable.

- **Complejidad de Relaciones**: Para modelar relaciones más complejas que no pueden ser capturadas por términos aditivos simples.

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 (x_1\cdot x_2)  + \epsilon
$$
Aquí $\beta_3$ representa la interacción de $X_1$ y $X_2$.

## Regresión lineal múltiple: X numéricas

```{r}
#| echo: false
summary(lm(Petal.Length ~ Sepal.Length + Sepal.Width, data = iris))
```

::: nonincremental
- Por cada unidad de aumento de $X$, $Y$ aumenta en $\beta_i$
 unidades
:::

## Regresión lineal múltiple: X factores

```{r}
#| echo: false
summary(lm(Petal.Length ~ Sepal.Length + Species, data = iris))
```

::: nonincremental
- Por el efecto de la categoría 2 de $X$ genera un aumento/disminución en $Y$ en $\beta_i$ unidades, respecto a la categoría 1 (nivel base, nivel de contraste, placebo).
 unidades
:::

## Interpretación de interacciones

- $\beta_1$ es el efecto de $X_1$ en $Y$ cuando $X_2 = 0$.
- $\beta_2$ es el efecto de $X_2$ en $Y$ cuando $X_1 = 0$.
- $\beta_3$ es el cambio en el efecto de $X_1$ sobre $Y$ por cada unidad adicional de $X_2$ y viceversa.

```{r}
#| echo: false
summary(lm(Petal.Length ~ Sepal.Length * Sepal.Width, data = iris))
```

## Métricas de evaluación y comparación de modelos

```{r}
#| echo: false

library(gt)
library(tibble)

# Crear el dataframe con las métricas de evaluación
datos_metricas_regresion <- tibble(
  Métrica = c(
    "RSE (Residual Standard Error)",
    "RMSE (Root Mean Squared Error)",
    "R-cuadrado (R²)",
    "R-cuadrado Ajustado (R²_adj)",
    "MAE (Mean Absolute Error)",
    "MSE (Mean Squared Error)",
    "AIC (Akaike Information Criterion)",
    "BIC (Bayesian Information Criterion)",
    "MAPE (Mean Absolute Percentage Error)"
  ),
  Descripción = c(
    "Raíz de la suma de residuos al cuadrado dividida por los grados de libertad.",
    "Raíz cuadrada del promedio de los residuos al cuadrado.",
    "Proporción de la variabilidad en Y explicada por los predictores.",
    "R² ajustado por el número de predictores, penalizando la inclusión de variables irrelevantes.",
    "Promedio de las diferencias absolutas entre observados y predichos.",
    "Promedio de los residuos al cuadrado.",
    "Métrica basada en la verosimilitud penalizada por la complejidad del modelo.",
    "Similar a AIC pero con penalización más fuerte para la complejidad.",
    "Promedio de los errores absolutos expresados como porcentaje de Y."
  ),
  Ventajas = c(
    "Ajusta el error por la complejidad del modelo.",
    "Intuitivo y ampliamente usado.",
    "Fácil de interpretar.",
    "Mejora la comparabilidad entre modelos con diferente complejidad.",
    "Robusto ante outliers.",
    "Utilizado en muchos métodos de optimización.",
    "Útil para comparar modelos no anidados.",
    "Preferido cuando se busca un balance más estricto entre ajuste y simplicidad.",
    "Intuitivo en términos relativos."
  ),
  Limitaciones = c(
    "No es tan intuitivo fuera del contexto de regresión.",
    "Sensible a outliers.",
    "No penaliza la complejidad del modelo.",
    "Menos intuitivo que R².",
    "No penaliza errores grandes de manera diferenciada.",
    "Sensible a outliers.",
    "Puede favorecer modelos más simples de forma excesiva.",
    "Puede ser demasiado restrictivo con muchos predictores.",
    "No definido cuando Y = 0 o cerca de cero."
  )
)

# Crear la tabla con gt
tabla_metricas_regresion <- datos_metricas_regresion %>%
  gt() %>%
  # Estilo de negrita para la columna 'Métrica'
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(
      columns = c(Métrica)
    )
  ) %>%
  # Alinear el texto a la izquierda para las demás columnas
  cols_align(
    align = "left",
    columns = c(Descripción, Ventajas, Limitaciones)
  ) %>%
  # Estilo de negrita para los encabezados de las columnas
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) %>%
  # Añadir franjas de color alternadas en las filas
  opt_row_striping() %>%
  # Configuración de opciones de la tabla
  tab_options(
    column_labels.background.color = "#0013fe",  # Color de fondo de los encabezados
    column_labels.font.size = px(25),             # Tamaño de fuente de los encabezados
    table.font.size = px(21),                      # Tamaño de fuente de los datos
    data_row.padding = px(8)                       # Padding de las filas de datos
  ) %>%
  # Añadir CSS personalizado para cambiar el color de fondo al pasar el cursor
  opt_css(
    css = "
    .gt_table tr:hover td {
      background-color: #E6F3FF !important;
    }
    "
  ) 

# Mostrar la tabla
tabla_metricas_regresion

```


## Modelos logarítmicos 

:::{.nonincremental}

- **Lineal-lineal** (no logarítmico): regresión clásica

$$
y = \beta_0 + \beta_1 x_1 + \epsilon
$$

- **Lineal-log**: X logaritmizada

$$
y = \beta_0 + \beta_1 \log(x_1) + \epsilon
$$

- **Log-lineal**: Y logaritmizada

$$
\log(Y) = \beta_0 + \beta_1 x_1 + \epsilon
$$

- **Log-Log**: X e Y logaritmizada

$$
\log(Y) = \beta_0 + \beta_1 \log(x_1) + \epsilon
$$
:::

## Interpretaciones de resultados en modelos logarítmicos

```{r}
#| echo: false

  
library(gt)
library(tibble)

# Crear el dataframe
datos_modelos_regresion <- tibble(
  Modelo = c("Lineal Clásico", "Log-Linear", "Lineal-Log", "Log-Log"),
  Ecuación = c(
    "Y = β0 + β1X + ϵ",
    "log(Y) = β0 + β1X + ϵ",
    "Y = β0 + β1 log(X) + ϵ",
    "log(Y) = β0 + β1 log(X) + ϵ"
  ),
  Interpretación_β1 = c(
    "Por cada unidad de aumento en X, se espera que Y aumente en β1 unidades.",
    "Por cada unidad de aumento en X, se espera que Y aumente por un factor F = exp(β1). Si F>1, entonces aumenta en F-1 x 100. Si F<1, entonces disminuye en 1-F x 100.",
    "Por cada aumento porcentual en X (digamos POR = 0.01), se espera que Y aumente en β1 x POR, o  β1 x 0.01 unidades.",
    "Por cada cambio porcentual en X (digamos POR% = 1%), se espera que Y cambie aproximadamente en β1xPOR%."
  )
)

# Crear la tabla con gt
tabla_modelos_regresion <- datos_modelos_regresion %>%
  gt() %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(
      columns = c(Modelo)
    )
  )%>%
  cols_align(
    align = "center",
    columns = c(`Ecuación`, `Interpretación_β1`)
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) %>%
  opt_row_striping() %>%
  tab_options(
    column_labels.background.color = "#0013fe",
    table.font.size = px(28),
    data_row.padding = px(15)
  ) %>%
  opt_css(
    css = "
    .gt_table tr:hover td {
      background-color: #E6F3FF !important;
    }
    "
  )

# Mostrar la tabla
tabla_modelos_regresion

```


